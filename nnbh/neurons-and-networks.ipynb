{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons and Networks\n",
    "\n",
    "With the pre-requisite math established, we can now start considering the question: what actually is a 'neural network' anyway? If you're reading this page, there's a good chance you've seen a diagram that looks something like this:\n",
    "\n",
    "![network-diagram](../images/network-diagram.png)\n",
    "\n",
    "But what is this diagram actually showing us?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Neuron\n",
    "\n",
    "Before considering a whole neural network, it's worth looking at a single *node* in the network. Looking at the above plot, a neuron is simple a single node in the *hidden* or *output* *layers* in the plot (more on these later). In particular, we're going to consider the first neuron from the first hidden layer:\n",
    "\n",
    "![neuron-diagram](../images/neuron-diagram.png)\n",
    "\n",
    "For simplicity, we'll consider what happens to a single observation $\\boldsymbol{X'} = [X_1, X_2, X_3]$.\n",
    "\n",
    "So, $X_1$, $X_2$, and $X_3$ go in, something happens and $a_1$ is made, and something goes out.\n",
    "\n",
    "Let's start at 'something happens' Each of the inputs to the node is mupltiplied by a weight: $w_{11}^{(1)}$ for $X_1$, $w_{12}^{(1)}$ for $X_2$, and $w_{13}^{(1)}$ for $X_3$. Here, the subscript I'm using for each $w$ denotes the input node (second number) and output node (first number). The superscript denotes which layer the weight belongs to. We can write this in matrix form as:\n",
    "\n",
    "$$ \\boldsymbol{W}_1^{(1)}\\boldsymbol{X'} $$\n",
    "\n",
    "We also usually add a *bias* (i.e. intercept) to this equation for each node, and it's useful to assign a the output to a variable. Let's call this $Z_1^{(1)}$:\n",
    "\n",
    "$$ Z_1^{(1)} = \\boldsymbol{W}^{(1)}_1\\boldsymbol{X'} + b_1^{(1)} $$\n",
    "\n",
    "To get $a_1^{(1)}$ from $Z_1^{(1)}$, we apply an *activation function* $f(x)$. Don't worry too much about the choice of function for now: there are many valid choices. The important think to note is that this function will take the weighted input $Z$, and transform it in some way:\n",
    "\n",
    "$$ a_1^{(1)} = f(Z_1^{(1)}) = f(\\boldsymbol{W}^{(1)}_1\\boldsymbol{X'} + b_1^{(1)}) $$\n",
    "\n",
    "The neuron then provides $a_1^{(1)}$ as its output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Layer\n",
    "\n",
    "We can easily generalise from a single neuron with a single output to many neurons with many outputs:\n",
    "\n",
    "$$ \\boldsymbol{Z}^{(1)} = \\boldsymbol{W}^{(1)}\\boldsymbol{X'} + \\boldsymbol{b}^{(1)} $$\n",
    "$$ \\boldsymbol{a}^{(1)} = f(\\boldsymbol{Z}^{(1)}) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many Hidden Layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b3010db54221e39668d7fe34b06214f54669d432870c9edb2f6a865eeb971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
