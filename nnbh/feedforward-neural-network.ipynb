{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "With all of the prior material established, I can now turn to the goal of this jupyter book: writing a general feedforward neural network (FNN) function. This function won't be particularly optimised - indeed, it's fairly slow in practice. It does however work.\n",
    "\n",
    "Since neural networks by their nature are fairly modular, I'll be taking a modular approach to programming this function. I'll start by writing the code for various activation functions, cost functions, their derivatives, weight initialisation functions, and functions to mananage which ones to use given an input string.\n",
    "\n",
    "From there, I'll write an FNN function that uses all of these internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "First, I'll implement the activation functions and their derivatives.\n",
    "\n",
    "Note that we don't really need the softmax derivative as the derivative of the cost with respect to the output nodes' weighted inputs is a known quantity. Because of this, there is a placeholder function that returns None (this is a little inefficient - but avoids me having to rewrite the whole network!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise sigmoid derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise softmax function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    shiftx = x - np.max(x) #creates numerical stability w/out changing result\n",
    "    expx = np.exp(shiftx)\n",
    "    return expx / np.sum(expx, axis=0)\n",
    "\n",
    "def deriv_softmax(x: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Placeholder function for softmax derivative. Not necessary because of\n",
    "    the formula for dCost/dZ for the output layer\n",
    "    \"\"\"\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise tanh function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def deriv_tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise tanh derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return 1 - np.power(tanh(x),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return x * (x>=0)\n",
    "\n",
    "def deriv_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise ReLU derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return (x>=0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise linear function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return x\n",
    "\n",
    "def deriv_linear(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise linear derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return np.ones(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions\n",
    "\n",
    "Next, the cost functions and their derivatives. We don't actually need to compute the cost to estimate a neural network, but I think it's useful to demonstrate how it can be done.\n",
    "\n",
    "As with softmax, we also don't need the derivative of cross-entropy as the derivative of the cost with respect to the output nodes' weighted inputs is a known quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MSE cost function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of costs\n",
    "    \"\"\"\n",
    "    return np.power(yhat-y,2)\n",
    "\n",
    "def deriv_mse(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MSE cost derivative function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of cost derviatives\n",
    "    \"\"\"\n",
    "    return 2*(yhat-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary cross entropy cost function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of costs\n",
    "    \"\"\"\n",
    "    return -y * np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "\n",
    "def deriv_binary_cross_entropy(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary cross entropy cost derivatives function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of cost derivatives\n",
    "    \"\"\"\n",
    "    return -(y/yhat) + (1-y)/(1-yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross entropy cost function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of costs\n",
    "    \"\"\"\n",
    "    return np.sum(-y*np.log(yhat),axis=0)\n",
    "\n",
    "def cross_entropy_deriv(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Placeholder cross entropy cost derivatives function. Not required\n",
    "    in practice as softmax dZ term is known\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of cost derivatives\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Activation and Cost Function Choice\n",
    "\n",
    "To aid in managing all of these, I also wrote some functions that return the relevant functions based on an input string. This means the final FNN function will take a string as input to describe the hidden and output activations. I could also have passed the functions directly, but this helped render the code a little clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which activation function and\n",
    "    derivative to use given input string\n",
    "\n",
    "    Args:\n",
    "        name: the name of the activation function\n",
    "\n",
    "    Returns: The activation function and its derivative\n",
    "    \"\"\"\n",
    "    if name == \"sigmoid\":\n",
    "        return sigmoid, deriv_sigmoid\n",
    "    if name == \"softmax\":\n",
    "        return softmax, deriv_softmax\n",
    "    if name == \"tanh\":\n",
    "        return tanh, deriv_tanh\n",
    "    if name == \"relu\":\n",
    "        return relu, deriv_relu\n",
    "    if name == \"linear\":\n",
    "        return linear, deriv_linear\n",
    "\n",
    "def get_cost(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which cost function and\n",
    "    derivative to use given input string\n",
    "\n",
    "    Args:\n",
    "        name: the name of the cost function\n",
    "\n",
    "    Returns: The cost function and its derivative\n",
    "    \"\"\"\n",
    "    if name == \"mse\":\n",
    "        return mse, deriv_mse\n",
    "    if name == \"bce\":\n",
    "        return binary_cross_entropy, deriv_binary_cross_entropy\n",
    "    if name == \"ce\":\n",
    "        return cross_entropy, cross_entropy_deriv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialisation\n",
    "\n",
    "An important step in setting up a neural network is initialising the weights. The network won't be successfully estimated if we initialise the weights at 0, and it's possible to initialise weights in such a manner that exploding or shrinking gradients are encouraged from the start. To avoid that outcome, I've therefore implemented some established weight initialisation algorithms here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape: tuple[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Xavier initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) / num_input\n",
    "    return W\n",
    "\n",
    "def he_init(shape: tuple[int]):\n",
    "    \"\"\"\n",
    "    He initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) * np.sqrt(2/num_input)\n",
    "    return W\n",
    "\n",
    "def linear_init(shape: tuple[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input)/num_input\n",
    "    return W\n",
    "\n",
    "def get_init(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which weights initalisation\n",
    "    to use\n",
    "\n",
    "    Args:\n",
    "        name: the name of the activation function\n",
    "\n",
    "    Returns: An initialisation function\n",
    "    \"\"\"\n",
    "    if name == \"sigmoid\" or name == \"softmax\" or name == \"tanh\":\n",
    "        return xavier_init\n",
    "    if name == \"relu\":\n",
    "        return he_init\n",
    "    if name == \"linear\":\n",
    "        return linear_init\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Here I implement the forward propagation algorithm. This takes the input features matrix X, the weights, the biases, and the activiation function, and propagates the data forward through the network to produce a prediction.\n",
    "\n",
    "Note that since A_list stores X as its first entry for ease of computation for both the forward and backward passes, its indices correspond to the hidden layers (i.e. A\\[i\\] would correspond to the ith hidden layer). So, on the fisrt part of the loop (i=0), A\\[i\\] would be A\\[0\\] and thus correspond to X. While Z\\[i\\] = Z\\[0\\] would be for i+1 - i.e. the first layer's weighted input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X: np.ndarray,\n",
    "                 W_list: list[np.ndarray],\n",
    "                 B_list: list[np.ndarray],\n",
    "                 activation_hidden: Callable,\n",
    "                 activation_output: Callable,\n",
    "                 num_pass: int):\n",
    "    \"\"\"\n",
    "    A function that performs forward-propagation through a network\n",
    "\n",
    "    Args:\n",
    "        X: matrix of input features\n",
    "        weight_list: A list with num_features_out by num_features_in weight\n",
    "                     matrices, where num_features_out is the number of outputs\n",
    "                     to the layer and num_features_in is the number of inputs to\n",
    "                     the layer\n",
    "        bias_list: A list with num_features_out by 1 bias matrices, where\n",
    "                   num_features_out is the number of outputs to the layer\n",
    "        activation_hidden: Activation function for hidden layers\n",
    "        activation_output: Activation function for the output layer\n",
    "        num_pass: Number of forward passes\n",
    "\n",
    "    Returns: Z_list, a list of layer weighted inputs, and A_list, a list of\n",
    "             activated weighted inputs. Both contain X as their first entry\n",
    "    \"\"\"\n",
    "    Z_list = []\n",
    "    A_list = [X] #for use in backprop\n",
    "    for i in range(num_pass):\n",
    "        Z_list.append(np.dot(W_list[i],A_list[i]) + B_list[i])\n",
    "        if i < (num_pass-1):\n",
    "            A_list.append(activation_hidden(Z_list[i]))\n",
    "        elif i == (num_pass-1):\n",
    "            A_list.append(activation_output(Z_list[i]))\n",
    "\n",
    "    return Z_list, A_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "And finally, here I implement the backpropagation algorthim. Note the slightly different implementations for dCost/dZ for the output layer depending on if softmax is used or not.\n",
    "\n",
    "As with forward propagation, the A_list contains X as an additional element - and so its indices behave slightly different to the other lists' indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_pass(Y: np.ndarray,\n",
    "              W_list: list[np.ndarray],\n",
    "              Z_list: list[np.ndarray],\n",
    "              A_list: list[np.ndarray],\n",
    "              cost_deriv: Callable,\n",
    "              activation_hidden_deriv: Callable,\n",
    "              activation_output_deriv: Callable,\n",
    "              num_pass: int,\n",
    "              num_obs: int,\n",
    "              softmax: bool):\n",
    "    \"\"\"\n",
    "    A function to perform backpropagation through the network\n",
    "\n",
    "    Args:\n",
    "        Y: array of output variables\n",
    "        W_list: list of layer weights\n",
    "        Z_list: list of layer weighted inputs\n",
    "        A_list: list of layer activations\n",
    "        cost_deriv: Cost derivative function\n",
    "        activation_hidden_deriv: Hidden layer activation func derivative\n",
    "        activation_output_deriv: Output layer activation func derivative\n",
    "        num_pass: Number of forward/backward passes\n",
    "        num_obs: Number of training examples/observations\n",
    "\n",
    "    Returns: dW_list, a list of weight derivatives, and dB_list, a list of\n",
    "             bias derivatives\n",
    "    \"\"\"\n",
    "    if softmax:\n",
    "        dZ = A_list[num_pass] - Y\n",
    "    if not softmax: \n",
    "        dZ = np.multiply(cost_deriv(A_list[num_pass], Y), activation_output_deriv(Z_list[num_pass-1]))\n",
    "    dW_list = [np.dot(dZ, A_list[num_pass-1].T)/num_obs]\n",
    "    dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs]\n",
    "    for i in range(num_pass-1, 0, -1):\n",
    "        dZ = np.multiply(np.dot(W_list[i].T, dZ), activation_hidden_deriv(Z_list[i-1]))\n",
    "        dW_list = [np.dot(dZ, A_list[i-1].T)/num_obs] + dW_list\n",
    "        dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs] + dB_list\n",
    "    return dW_list, dB_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Feedforward Neural Network Function\n",
    "\n",
    "With all of the above functions impelemented, it's time to put them all together. This function does a few things:\n",
    "\n",
    "- Puts data in desired format (num_features by num_observations, instead of vice versa)\n",
    "- Uses the functions above to prepare the activation functions, cost function, and their derivatives\n",
    "- Runs a training loop. During a single cycle of the loop, it will:\n",
    "  - Forward-propagate the data to calculate the weighted inputs and activations\n",
    "  - Back-propagate to compute the gradients of the weights and the biases\n",
    "  - Update the weights according to gradient descent \n",
    "- This loop has two break conditions:\n",
    "  - The max number of iterations is reached\n",
    "  - The change in weights becomes sufficiently small\n",
    "\n",
    "Finally, the function produces three outputs, two of which I'll use in this jupyter book:\n",
    "\n",
    "- The network's predicted values for the input data\n",
    "- The overall cost of the network\n",
    "- A function built from the forward propagation function above that will generate predictions for new data\n",
    "\n",
    "There's no reason I coudln't have included other outputs such as the estimated weights, the final difference, or the final number of iterations. I don't use the cost here, but since it's not actually used in estimating the network I didn't want it to be entirely superfluous to requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fnn(X,\n",
    "        Y,\n",
    "        hidden,\n",
    "        learn_rate=0.03,\n",
    "        hidden_activation=\"relu\",\n",
    "        output_activation=\"linear\",\n",
    "        loss=\"mse\",\n",
    "        tol=0.0001,\n",
    "        max_iter=10000):\n",
    "    \"\"\"\n",
    "    A feedforward neural network function that allows an arbitary number of \n",
    "    hidden layers and \n",
    "\n",
    "    Args:\n",
    "        X: A num_obs by num_features input matrix\n",
    "        Y: A num_obs by num_outputs matrix\n",
    "        hidden: A list of length num_hidden_layers where the nth entry describes\n",
    "                the number of nodes for the nth hidden layer.\n",
    "        learn_rate: Learning rate of the network.\n",
    "        hidden_activation: String denoting activation function for hidden layers.\n",
    "        output_activation: String denoting activation function for output layer.\n",
    "        loss: String denoting the loss function\n",
    "        tol: Minimum difference for break criteria in the training loop\n",
    "        max_iter: Maximum number of training iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data will be aligned in a different format to the network\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    # Initialise network: core numbers\n",
    "    shape_X = X.shape\n",
    "    num_obs = shape_X[1]\n",
    "    indices = list(range(num_obs))\n",
    "    num_features = shape_X[0]\n",
    "    shape_Y = Y.shape\n",
    "    num_output = shape_Y[0]\n",
    "    num_hidden = len(hidden)\n",
    "    num_pass = num_hidden + 1 #passes between layers\n",
    "    \n",
    "    # Initialise network: weights W:\n",
    "    hidden_init, output_init = get_init(hidden_activation), get_init(output_activation)\n",
    "    W_list =  [hidden_init((hidden[0], num_features))]\n",
    "    for i in range(1, num_hidden):\n",
    "        W_list.append(hidden_init((hidden[i], hidden[i-1])))\n",
    "    W_list.append(output_init((num_output, hidden[num_hidden-1])))\n",
    "    \n",
    "    # Initialise network: biases B\n",
    "    B_list = []\n",
    "    for i in range(num_hidden):\n",
    "        B_list.append(np.zeros((hidden[i],1)))\n",
    "    B_list.append(np.zeros((num_output, 1)))\n",
    "    \n",
    "    # Initalise network: activation + cost funcs and their derivatives\n",
    "    activation_hidden, activation_hidden_deriv = get_activation(hidden_activation)\n",
    "    activation_output, activation_output_deriv = get_activation(output_activation)\n",
    "    cost_func, cost_deriv = get_cost(loss)\n",
    "    softmax_bool = output_activation == \"softmax\"\n",
    "    \n",
    "    # Training loop\n",
    "    iter = 0\n",
    "    diff = np.inf\n",
    "    while (iter < max_iter and diff > tol):\n",
    "        # Compute forward pass\n",
    "        Z_list, A_list = forward_pass(X=X,\n",
    "                                      W_list=copy.deepcopy(W_list),\n",
    "                                      B_list=copy.deepcopy(B_list),\n",
    "                                      activation_hidden=activation_hidden,\n",
    "                                      activation_output=activation_output,\n",
    "                                      num_pass=num_pass)\n",
    "        \n",
    "        # Backpropagate to find gradients\n",
    "        dW_list, dB_list = back_pass(cost_deriv=cost_deriv,\n",
    "                                     Y=copy.deepcopy(Y),\n",
    "                                     activation_output_deriv=activation_output_deriv,\n",
    "                                     activation_hidden_deriv=activation_hidden_deriv,\n",
    "                                     W_list=copy.deepcopy(W_list),\n",
    "                                     A_list=copy.deepcopy(A_list),\n",
    "                                     Z_list=copy.deepcopy(Z_list),\n",
    "                                     num_pass=num_pass,\n",
    "                                     num_obs=num_obs,\n",
    "                                     softmax=softmax_bool)\n",
    "        \n",
    "        # Update params (w_new = w_old - learn_rate * gradient(w_old))\n",
    "        W_old = copy.deepcopy(W_list)\n",
    "        B_old = copy.deepcopy(B_list)\n",
    "        new_diff = 0\n",
    "        for i in range(num_pass):\n",
    "            W_list[i] = W_list[i] - (learn_rate * dW_list[i])\n",
    "            B_list[i] = B_list[i] - (learn_rate * dB_list[i])\n",
    "            new_diff += np.sum(np.abs(W_list[i] - W_old[i])) + np.sum(np.abs(B_list[i] - B_old[i]))\n",
    "        diff = new_diff\n",
    "\n",
    "        # Increment iterations\n",
    "        iter += 1\n",
    "    \n",
    "    # Predicted values of the network\n",
    "    prediction = A_list[num_pass].T\n",
    "    \n",
    "    # Cost of the network\n",
    "    cost = np.sum(cost_func(A_list[num_pass], Y)) / num_obs\n",
    "    \n",
    "    # Prediction function for new data\n",
    "    def fnn_pred_func(newdata):\n",
    "        new_X = newdata.T\n",
    "        _, A_list = forward_pass(X=new_X,\n",
    "                                 W_list=W_list,\n",
    "                                 B_list=B_list,\n",
    "                                 activation_hidden=activation_hidden,\n",
    "                                 activation_output=activation_output,\n",
    "                                 num_pass=num_pass)\n",
    "        return A_list[num_pass].T\n",
    "    \n",
    "    # Return Yhat\n",
    "    return prediction, cost, fnn_pred_func\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Learning XOR\n",
    "\n",
    "With the network function implemented, it's time to put it to the test. For the first test, I'll run it on XOR. If you're unfamiliar with XOR, it's a logical operation read as 'exclusive OR'. It takes two binary inputs. If one *but not the other* is true, then it returns true. If both inputs are false, or if both inputs are true, it returns false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty of XOR is that it's plainly non-linear to the inputs. Without using interactions, it would be impossible to model in any kind of linear model. The power of neural networks however is that they can approximate any function, including non-linear functions.\n",
    "\n",
    "In this case, all we need is a single hidden layer with 2 neurons. With this, we should have enough to estimate a reasonably accurate network.\n",
    "\n",
    "Since none of the inputs are negative, ReLU would be a poor choice of activation function, as with negative weights a neuron would become 'dead' (unless a different initialisation which prevented negative weights was used). I therefore chose a sigmoid activation for the hidden layer.\n",
    "\n",
    "Similarly, since the output layer is binary, I used a sigmoid output activation and binary cross-entropy loss function.\n",
    "\n",
    "Since all the possible feature combinations of XOR are known, I examined only the predicted output for the input data in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "pred, _, _ = fnn(X=X,\n",
    "                 Y=Y,\n",
    "                 hidden=[2],\n",
    "                 learn_rate=0.9,\n",
    "                 hidden_activation=\"sigmoid\",\n",
    "                 output_activation=\"sigmoid\",\n",
    "                 loss=\"bce\")\n",
    "print(np.round(pred,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network successfully estimates the correct values for XOR. While this demonstrates the network's ability to approximate non-linear functions through the use of hidden layers and appropriate activation functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Digit Classification\n",
    "\n",
    "XOR is a however fairly simple example - so let's turn to something more involved. I'm now going to test my FNN function on one of the sklearn toy datasets. In particular, I'm going to test it on the handwritten digits toy dataset.\n",
    "\n",
    "This is a dataset of 8x8 pixel images of digits ranging from 0 to 9. I'll attempt to classify the images based on the individual pixels. First, I'll download the data split it into training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "data, target = load_digits(n_class=10, return_X_y=True, as_frame=False)\n",
    "target = target.reshape((target.shape[0],1))\n",
    "\n",
    "# Split data\n",
    "indices = list(range(data.shape[0]))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, stratify=target)\n",
    "train_X = data[train_indices,:]\n",
    "train_Y = target[train_indices,:]\n",
    "test_X = data[test_indices,:]\n",
    "test_Y = target[test_indices,:]\n",
    "\n",
    "# One-hot encode the targets\n",
    "num_classes = 10\n",
    "def one_hot(vec):\n",
    "    oh = np.eye(num_classes)[vec].reshape((vec.shape[0],-1))\n",
    "    return oh\n",
    "train_Y_oh = one_hot(train_Y)\n",
    "test_Y_oh = one_hot(test_Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared, I'll now train a network on it. Unlike XOR, there is not a limited set of possible feature combinations, so this time the emphasis will be on how well the model predicts new data. I thus focus on the prediction function output.\n",
    "\n",
    "Since this dataset is more complex (64 features, 10 classes), I used a more complex network setup. In this case, a network with two hidden layers. The first hidden layer contains 20 neurons, the second 10 neurons, for a total of 40 hidden layer neurons.\n",
    "\n",
    "For similar reasons in the case of XOR, I used a sigmoid activation function for the hiddden layers. This time however, since our output is not one but many classes, I used a softmax output activation and cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3298)\n",
    "_, _, pred_func = fnn(X=train_X,\n",
    "                      Y=train_Y_oh,\n",
    "                      hidden=[20,10],\n",
    "                      learn_rate=0.9,\n",
    "                      hidden_activation=\"sigmoid\",\n",
    "                      output_activation=\"softmax\",\n",
    "                      loss=\"ce\",\n",
    "                      max_iter=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network estimated, I'll now get the predictions from the network. Since I don't want to compare against probabilities, I'll use the value the network considers likeliest given the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pred_func(test_X)\n",
    "prediction_vec = prediction.argmax(1,keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise this, let's look at the first image in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1474ad930>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = test_X[0]\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img.reshape(8,8), cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's compare this to our network's prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted number: {prediction_vec[0][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the correct value!\n",
    "\n",
    "Of course, we don't want to look only at a single value. So let's compute the perentage of the labels that were correctly predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_Y == prediction_vec) / len(test_Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of predictions are correct! Pretty good going as far as handwritten digit recognition goes.\n",
    "\n",
    "This concludes the demonstration of the feedforward neural network function. It works for arbitrarily sized data and for arbitarily sized neural networks, although it will probably be fairly slow in practice for larger datasets and networks. The ultimate goal of the juptyer book has therefore been realised."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b3010db54221e39668d7fe34b06214f54669d432870c9edb2f6a865eeb971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
