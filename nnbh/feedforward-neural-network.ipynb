{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "A full, general FNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from typing import Callable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - np.power(tanh(x),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x>=0)\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return (x>=0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def deriv_linear(x):\n",
    "    return np.ones(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat, y):\n",
    "    return np.power(yhat-y,2)\n",
    "\n",
    "def deriv_mse(yhat, y):\n",
    "    return 2*(yhat-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return -y * np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "\n",
    "def deriv_cross_entropy(yhat, y):\n",
    "    return -(y/yhat) + (1-y)/(1-yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Activation and Cost Function Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manage which activation function to use\n",
    "def get_activation(name: str) -> Callable:\n",
    "    if name == \"sigmoid\":\n",
    "        return sigmoid, deriv_sigmoid\n",
    "    if name == \"tanh\":\n",
    "        return tanh, deriv_tanh\n",
    "    if name == \"relu\":\n",
    "        return relu, deriv_relu\n",
    "    if name == \"linear\":\n",
    "        return linear, deriv_linear\n",
    "\n",
    "# Function to manage which cost function to use\n",
    "def get_cost(name: str) -> Callable:\n",
    "    if name == \"mse\":\n",
    "        return mse, deriv_mse\n",
    "    if name == \"ce\":\n",
    "        return cross_entropy, deriv_cross_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape):\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) / num_input\n",
    "    return W\n",
    "\n",
    "def he_init(shape):\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) * np.sqrt(2/num_input)\n",
    "    return W\n",
    "\n",
    "def linear_init(shape):\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input)/num_input\n",
    "    return W\n",
    "\n",
    "def get_init(name):\n",
    "    if name == \"sigmoid\" or name == \"tanh\":\n",
    "        return xavier_init\n",
    "    if name == \"relu\":\n",
    "        return he_init\n",
    "    if name == \"linear\":\n",
    "        return linear_init\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Note Z\\[i+1\\] is because we're storing X in the Z_list as well. i+1 should be understood as layer i!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(Z_list, W_list, B_list, activation_hidden,\n",
    "                 activation_output, num_pass):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Z_list (list[np.ndarray]): A list of num_features by num_obs\n",
    "                                                 matrices, where the number of\n",
    "                                                 features is the number of nodes\n",
    "                                                 on the previous layer\n",
    "        weight_list (list[np.ndarray]): A list of num_features_l by num_features_(l-1)\n",
    "                                        matrices, where num_features_l is the number\n",
    "                                        of features for the lth layer\n",
    "        bias_list (list[np.ndarray]): _description_\n",
    "        activation_hidden (_type_): _description_\n",
    "        activation_output (_type_): _description_\n",
    "        num_pass (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    A_list = [Z_list[0]] #for use in backprop\n",
    "    for i in range(num_pass):\n",
    "        Z_list[i+1] = np.dot(W_list[i],A_list[i]) + B_list[i]\n",
    "        if i < (num_pass-1):\n",
    "            A_list.append(activation_hidden(Z_list[i+1]))\n",
    "        elif i == (num_pass-1):\n",
    "            A_list.append(activation_output(Z_list[i+1]))\n",
    "\n",
    "    return Z_list, A_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_pass(cost_deriv,\n",
    "              Y,\n",
    "              activation_output_deriv,\n",
    "              activation_hidden_deriv,\n",
    "              W_list,\n",
    "              A_list,\n",
    "              Z_list,\n",
    "              num_pass,\n",
    "              num_obs):\n",
    "    # Docstring ends here\n",
    "    # dZ_list = [cost_deriv(A_list[num_pass], Y) * activation_output_deriv(Z_list[num_pass])]\n",
    "    dZ = np.multiply(cost_deriv(A_list[num_pass], Y), activation_output_deriv(Z_list[num_pass]))\n",
    "    dW_list = [np.dot(dZ, A_list[num_pass-1].T)/num_obs]\n",
    "    dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs]\n",
    "    for i in range(num_pass-1, 0, -1):\n",
    "        # dZ_list = [np.dot(W_list[i].T, dZ_list[0]) * activation_hidden_deriv(Z_list[i])] + dZ_list\n",
    "        dZ = np.multiply(np.dot(W_list[i].T, dZ), activation_output_deriv(Z_list[i]))\n",
    "        dW_list = [np.dot(dZ, A_list[i-1].T)/num_obs] + dW_list\n",
    "        dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs] + dB_list\n",
    "    return dW_list, dB_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Feedforward Neural Network Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnn(X, Y, hidden, learn_rate=0.03, hidden_activation=\"relu\",\n",
    "        output_activation=\"linear\", loss=\"mse\", batch_size=2, tol=0.0001, max_iter=10000):\n",
    "    \"\"\"\n",
    "    A feedforward neural network function that allows an arbitary number of \n",
    "    hidden layers and \n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): A num_features by num_obs matrix\n",
    "        Y (np.ndarray): A num_obs by num_outputs matrix\n",
    "        hidden (list[int]): A list of length num_hidden_layers where each\n",
    "                            entry describes the number of nodes for the nth\n",
    "                            hidden layer.\n",
    "        learn_rate (float): Learning rate of the network.\n",
    "        hidden_activation (str): Activation function for hidden layers.\n",
    "        output_activation (str): Activation function for output layer.\n",
    "        loss: loss function\n",
    "        tol (float): Minimum difference for ending training loop.\n",
    "        max_iter (int): Maximum number of training iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data will be aligned in a different format to the network\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    # Initialise network: core numbers\n",
    "    shape_X = X.shape\n",
    "    num_obs = shape_X[1]\n",
    "    indices = list(range(num_obs))\n",
    "    num_features = shape_X[0]\n",
    "    shape_Y = Y.shape\n",
    "    num_output = shape_Y[0]\n",
    "    num_hidden = len(hidden)\n",
    "    num_pass = num_hidden + 1 #passes between layers\n",
    "    \n",
    "    # Initialise network: weighted outputs of nodes Z\n",
    "    Z_list = [X]\n",
    "    for i in range(num_hidden):\n",
    "        Z_list.append(np.zeros((hidden[i], num_obs)))\n",
    "    Z_list.append(np.zeros((num_output, num_obs)))\n",
    "    \n",
    "    # Initialise network: weights W:\n",
    "    hidden_init, output_init = get_init(hidden_activation), get_init(output_activation)\n",
    "    W_list =  [hidden_init((hidden[0], num_features))]\n",
    "    for i in range(1, num_hidden):\n",
    "        W_list.append(hidden_init((hidden[i], hidden[i-1])))\n",
    "    W_list.append(output_init((num_output, hidden[num_hidden-1])))\n",
    "    \n",
    "    # Initialise network: biases B\n",
    "    B_list = []\n",
    "    for i in range(num_hidden):\n",
    "        B_list.append(np.zeros((hidden[i],1)))\n",
    "    B_list.append(np.zeros((num_output, 1)))\n",
    "    \n",
    "    # Initalise network: activation + cost funcs and their derivatives\n",
    "    activation_hidden, activation_hidden_deriv = get_activation(hidden_activation)\n",
    "    activation_output, activation_output_deriv = get_activation(output_activation)\n",
    "    cost_func, cost_deriv = get_cost(loss)\n",
    "    \n",
    "    # Training loop\n",
    "    iter = 0\n",
    "    diff = np.inf\n",
    "    while (iter <= max_iter and diff > tol):\n",
    "        # Compute forward pass\n",
    "        Z_list, A_list = forward_pass(Z_list=copy.deepcopy(Z_list),\n",
    "                                      W_list=copy.deepcopy(W_list),\n",
    "                                      B_list=copy.deepcopy(B_list),\n",
    "                                      activation_hidden=activation_hidden,\n",
    "                                      activation_output=activation_output,\n",
    "                                      num_pass=num_pass)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = cost_func(A_list[num_pass], Y) #individual costs\n",
    "        \n",
    "        # Backpropagate to find gradients\n",
    "        batch_indices = list(np.random.choice(num_obs, batch_size, replace=False))\n",
    "        dW_list, dB_list = back_pass(cost_deriv=cost_deriv,\n",
    "                                     Y=copy.deepcopy(Y[:,batch_indices]),\n",
    "                                     activation_output_deriv=activation_output_deriv,\n",
    "                                     activation_hidden_deriv=activation_hidden_deriv,\n",
    "                                     W_list=copy.deepcopy(W_list),\n",
    "                                     A_list=copy.deepcopy([A[:,batch_indices] for A in A_list]),\n",
    "                                     Z_list=copy.deepcopy([Z[:,batch_indices] for Z in Z_list]),\n",
    "                                     num_pass=num_pass,\n",
    "                                     num_obs=batch_size)\n",
    "        \n",
    "        \n",
    "        # Check the gradients - slow but worthwhile\n",
    "        epsilon = 1e-4\n",
    "        cost_plus = []\n",
    "        cost_minus = []\n",
    "        num_grads = []\n",
    "        diff_grads = []\n",
    "        for i in range(num_pass):\n",
    "            cost_plus.append(np.zeros(W_list[i].shape))\n",
    "            cost_minus.append(np.zeros(W_list[i].shape))\n",
    "            num_grads.append(np.zeros(W_list[i].shape))\n",
    "            diff_grads.append(np.zeros(W_list[i].shape))\n",
    "            for idx, x in np.ndenumerate(W_list[i]):\n",
    "                W_plus = copy.deepcopy(W_list)\n",
    "                W_plus[i][idx] = W_plus[i][idx] + epsilon\n",
    "                W_minus = copy.deepcopy(W_list)\n",
    "                W_minus[i][idx] = W_minus[i][idx] - epsilon\n",
    "       \n",
    "                _, A_plus = forward_pass(Z_list=copy.deepcopy(Z_list),\n",
    "                                        W_list=copy.deepcopy(W_plus),\n",
    "                                        B_list=copy.deepcopy(B_list),\n",
    "                                        activation_hidden=activation_hidden,\n",
    "                                        activation_output=activation_output,\n",
    "                                        num_pass=num_pass)\n",
    "                _, A_minus = forward_pass(Z_list=copy.deepcopy(Z_list),\n",
    "                                        W_list=copy.deepcopy(W_minus),\n",
    "                                        B_list=copy.deepcopy(B_list),\n",
    "                                        activation_hidden=activation_hidden,\n",
    "                                        activation_output=activation_output,\n",
    "                                        num_pass=num_pass)\n",
    "                cost_plus[i][idx] = np.sum(cost_func(A_plus[num_pass], Y))\n",
    "                cost_minus[i][idx] = np.sum(cost_func(A_minus[num_pass], Y))\n",
    "                num_grads[i][idx] = (cost_plus[i][idx] - cost_minus[i][idx])/(2*epsilon)\n",
    "                diff_grads[i][idx] = np.abs(num_grads[i][idx] - dW_list[i][idx])\n",
    "        \n",
    "        # print(diff_grads)\n",
    "        grad_tests = []\n",
    "        for i in range(num_pass):\n",
    "            grad_tests.append(np.isclose(num_grads[i], dW_list[i]))\n",
    "        # print(grad_tests)\n",
    "        \n",
    "        # Update weights (w_new = w_old - learn_rate * gradient(w_old))\n",
    "        W_old = copy.deepcopy(W_list)\n",
    "        B_old = copy.deepcopy(B_list)\n",
    "        new_diff = 0\n",
    "        for i in range(num_pass):\n",
    "            W_list[i] = W_list[i] - (learn_rate * num_grads[i]) #(learn_rate * dW_list[i])\n",
    "            B_list[i] = B_list[i] - (learn_rate * dB_list[i])\n",
    "            new_diff += np.sum(np.abs(W_list[i] - W_old[i])) + np.sum(np.abs(B_list[i] - B_old[i]))\n",
    "        diff = new_diff\n",
    "        # print(dW_list)\n",
    "        # print(Z_list[num_pass])\n",
    "        # print(A_list[num_pass])\n",
    "        # print(A_list[num_pass].shape)\n",
    "        # print(Y.shape)\n",
    "        # print(cost_func)\n",
    "        # print(cost)\n",
    "        # print(W_list)\n",
    "        # print(cost.shape)\n",
    "        # print(np.sum(cost))\n",
    "\n",
    "        # Increment iterations\n",
    "        iter += 1\n",
    "    \n",
    "    # Predicted values of the network\n",
    "    prediction = A_list[num_pass].T\n",
    "    \n",
    "    # Prediction function\n",
    "    def fnn_pred_func(X):\n",
    "        X = X.T,\n",
    "        _, A_list = forward_pass(Z_list=[X] + [[]]*(num_hidden+1),\n",
    "                                      W_list=copy.deepcopy(W_list),\n",
    "                                      B_list=copy.deepcopy(B_list),\n",
    "                                      activation_hidden=activation_hidden,\n",
    "                                      activation_output=activation_output,\n",
    "                                      num_pass=num_pass)\n",
    "        return A_list[num_pass]\n",
    "    \n",
    "    # print(iter <= max_iter and diff > tol)\n",
    "    # print(iter <= max_iter)\n",
    "    # print(diff > tol)\n",
    "    # print(num_obs)\n",
    "    \n",
    "    # Return Yhat\n",
    "    return prediction, iter, diff, np.sum(cost), fnn_pred_func\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03]\n",
      " [0.95]\n",
      " [0.95]\n",
      " [0.05]]\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "pred, iter, diff, cost, func = fnn(X=X,\n",
    "                                   Y=Y,\n",
    "                                   hidden=[2],\n",
    "                                   learn_rate=0.9,\n",
    "                                   batch_size=2,\n",
    "                                   hidden_activation=\"sigmoid\",\n",
    "                                   output_activation=\"sigmoid\",\n",
    "                                   loss=\"mse\",\n",
    "                                   max_iter=1000)\n",
    "print(np.round(pred,2))\n",
    "print(pred.shape)\n",
    "# print(pred)\n",
    "# print(iter)\n",
    "# print(diff)\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.47427244]\n",
      "  [0.47331199]]]\n"
     ]
    }
   ],
   "source": [
    "print(func(np.array([[0,1]])))\n",
    "# print(func(np.array([[1,1]])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.datasets\n",
    "# data_X, data_Y =  sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.model_selection\n",
    "# indices = list(range((X.shape[0])))\n",
    "# train_indices, test_indices = sklearn.model_selection.train_test_split(indices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, _, _, func_bc = fnn(X=data_X[train_indices,],\n",
    "#                        Y=data_Y[train_indices],\n",
    "#                         hidden=[10,8,6],\n",
    "#                         learn_rate=0.05,\n",
    "#                         hidden_activation=\"sigmoid\",\n",
    "#                         output_activation=\"sigmoid\",\n",
    "#                         loss=\"mse\",\n",
    "#                         max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = func_bc(data_X[test_indices,])\n",
    "# print(test_pred)\n",
    "# print(Y[test_indices])\n",
    "# # print(test_pred - Y[test_indices])\n",
    "# test_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b3010db54221e39668d7fe34b06214f54669d432870c9edb2f6a865eeb971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
