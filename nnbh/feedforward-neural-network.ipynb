{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "A full, general FNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from typing import Callable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "First, we'll implement the activation functions and their derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise sigmoid derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise tanh function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def deriv_tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise tanh derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return 1 - np.power(tanh(x),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return x * (x>=0)\n",
    "\n",
    "def deriv_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise ReLU derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return (x>=0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise linear function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return x\n",
    "\n",
    "def deriv_linear(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Element-wise linear derivative function\n",
    "\n",
    "    Args:\n",
    "        x: array of values\n",
    "\n",
    "    Returns: array of transformed values of the same shape as x\n",
    "    \"\"\"\n",
    "    return np.ones(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions\n",
    "\n",
    "Next, we'll implement the activation functions and their derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MSE cost function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of costs\n",
    "    \"\"\"\n",
    "    return np.power(yhat-y,2)\n",
    "\n",
    "def deriv_mse(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MSE cost derivative function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of cost derviatives\n",
    "    \"\"\"\n",
    "    return 2*(yhat-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary cross entropy cost function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of costs\n",
    "    \"\"\"\n",
    "    return -y * np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "\n",
    "def deriv_binary_cross_entropy(yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary cross entropy cost derivatives function\n",
    "\n",
    "    Args:\n",
    "        yhat: Array of predicted values\n",
    "        y: Array of true values\n",
    "\n",
    "    Returns: An array of cost derivatives\n",
    "    \"\"\"\n",
    "    return -(y/yhat) + (1-y)/(1-yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Activation and Cost Function Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which activation function and\n",
    "    derivative to use given input string\n",
    "\n",
    "    Args:\n",
    "        name: the name of the activation function\n",
    "\n",
    "    Returns: The activation function and its derivative\n",
    "    \"\"\"\n",
    "    if name == \"sigmoid\":\n",
    "        return sigmoid, deriv_sigmoid\n",
    "    if name == \"tanh\":\n",
    "        return tanh, deriv_tanh\n",
    "    if name == \"relu\":\n",
    "        return relu, deriv_relu\n",
    "    if name == \"linear\":\n",
    "        return linear, deriv_linear\n",
    "\n",
    "def get_cost(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which cost function and\n",
    "    derivative to use given input string\n",
    "\n",
    "    Args:\n",
    "        name: the name of the cost function\n",
    "\n",
    "    Returns: The cost function and its derivative\n",
    "    \"\"\"\n",
    "    if name == \"mse\":\n",
    "        return mse, deriv_mse\n",
    "    if name == \"bce\":\n",
    "        return binary_cross_entropy, deriv_binary_cross_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape: tuple[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Xavier initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) / num_input\n",
    "    return W\n",
    "\n",
    "def he_init(shape: tuple[int]):\n",
    "    \"\"\"\n",
    "    He initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input) * np.sqrt(2/num_input)\n",
    "    return W\n",
    "\n",
    "def linear_init(shape: tuple[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple initialisation of weights\n",
    "\n",
    "    Args:\n",
    "        shape: tuple giving the shape of the weights\n",
    "\n",
    "    Returns: np.ndarray of initialised weights\n",
    "    \"\"\"\n",
    "    num_input = shape[1]\n",
    "    num_output = shape[0]\n",
    "    W = np.random.randn(num_output, num_input)/num_input\n",
    "    return W\n",
    "\n",
    "def get_init(name: str) -> Callable:\n",
    "    \"\"\"\n",
    "    A function to manage which weights initalisation\n",
    "    to use\n",
    "\n",
    "    Args:\n",
    "        name: the name of the activation function\n",
    "\n",
    "    Returns: An initialisation function\n",
    "    \"\"\"\n",
    "    if name == \"sigmoid\" or name == \"tanh\":\n",
    "        return xavier_init\n",
    "    if name == \"relu\":\n",
    "        return he_init\n",
    "    if name == \"linear\":\n",
    "        return linear_init\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Note Z\\[i+1\\] is because we're storing X in the Z_list as well. i+1 should be understood as layer i!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X: np.ndarray,\n",
    "                 W_list: list[np.ndarray],\n",
    "                 B_list: list[np.ndarray],\n",
    "                 activation_hidden: Callable,\n",
    "                 activation_output: Callable,\n",
    "                 num_pass: int):\n",
    "    \"\"\"\n",
    "    A function that performs forward-propagation through a network\n",
    "\n",
    "    Args:\n",
    "        X: matrix of input features\n",
    "        weight_list: A list with num_features_out by num_features_in weight\n",
    "                     matrices, where num_features_out is the number of outputs\n",
    "                     to the layer and num_features_in is the number of inputs to\n",
    "                     the layer\n",
    "        bias_list: A list with num_features_out by 1 bias matrices, where\n",
    "                   num_features_out is the number of outputs to the layer\n",
    "        activation_hidden: Activation function for hidden layers\n",
    "        activation_output: Activation function for the output layer\n",
    "        num_pass: Number of forward passes\n",
    "\n",
    "    Returns: Z_list, a list of layer weighted inputs, and A_list, a list of\n",
    "             activated weighted inputs. Both contain X as their first entry\n",
    "    \"\"\"\n",
    "    Z_list = [X]\n",
    "    A_list = [X] #for use in backprop\n",
    "    for i in range(num_pass):\n",
    "        Z_list.append(np.dot(W_list[i],A_list[i]) + B_list[i])\n",
    "        if i < (num_pass-1):\n",
    "            A_list.append(activation_hidden(Z_list[i+1]))\n",
    "        elif i == (num_pass-1):\n",
    "            A_list.append(activation_output(Z_list[i+1]))\n",
    "\n",
    "    return Z_list, A_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_pass(Y: np.ndarray,\n",
    "              W_list: list[np.ndarray],\n",
    "              Z_list: list[np.ndarray],\n",
    "              A_list: list[np.ndarray],\n",
    "              cost_deriv: Callable,\n",
    "              activation_hidden_deriv: Callable,\n",
    "              activation_output_deriv: Callable,\n",
    "              num_pass: int,\n",
    "              num_obs: int):\n",
    "    \"\"\"\n",
    "    A function to perform backpropagation through the network\n",
    "\n",
    "    Args:\n",
    "        Y: array of output variables\n",
    "        W_list: list of layer weights\n",
    "        Z_list: list of layer weighted inputs\n",
    "        A_list: list of layer activations\n",
    "        cost_deriv: Cost derivative function\n",
    "        activation_hidden_deriv: Hidden layer activation func derivative\n",
    "        activation_output_deriv: Output layer activation func derivative\n",
    "        num_pass: Number of forward/backward passes\n",
    "        num_obs: Number of training examples/observations\n",
    "\n",
    "    Returns: dW_list, a list of weight derivatives, and dB_list, a list of\n",
    "             bias derivatives\n",
    "    \"\"\"\n",
    "    dZ = np.multiply(cost_deriv(A_list[num_pass], Y), activation_output_deriv(Z_list[num_pass]))\n",
    "    dW_list = [np.dot(dZ, A_list[num_pass-1].T)/num_obs]\n",
    "    dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs]\n",
    "    for i in range(num_pass-1, 0, -1):\n",
    "        dZ = np.multiply(np.dot(W_list[i].T, dZ), activation_output_deriv(Z_list[i]))\n",
    "        dW_list = [np.dot(dZ, A_list[i-1].T)/num_obs] + dW_list\n",
    "        dB_list = [np.sum(dZ, axis=1, keepdims=True)/num_obs] + dB_list\n",
    "    return dW_list, dB_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Feedforward Neural Network Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnn(X, Y, hidden, learn_rate=0.03, hidden_activation=\"relu\",\n",
    "        output_activation=\"linear\", loss=\"mse\", batch_size=2, tol=0.0001, max_iter=10000):\n",
    "    \"\"\"\n",
    "    A feedforward neural network function that allows an arbitary number of \n",
    "    hidden layers and \n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): A num_features by num_obs matrix\n",
    "        Y (np.ndarray): A num_obs by num_outputs matrix\n",
    "        hidden (list[int]): A list of length num_hidden_layers where each\n",
    "                            entry describes the number of nodes for the nth\n",
    "                            hidden layer.\n",
    "        learn_rate (float): Learning rate of the network.\n",
    "        hidden_activation (str): Activation function for hidden layers.\n",
    "        output_activation (str): Activation function for output layer.\n",
    "        loss: loss function\n",
    "        tol (float): Minimum difference for ending training loop.\n",
    "        max_iter (int): Maximum number of training iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data will be aligned in a different format to the network\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    # Initialise network: core numbers\n",
    "    shape_X = X.shape\n",
    "    num_obs = shape_X[1]\n",
    "    indices = list(range(num_obs))\n",
    "    num_features = shape_X[0]\n",
    "    shape_Y = Y.shape\n",
    "    num_output = shape_Y[0]\n",
    "    num_hidden = len(hidden)\n",
    "    num_pass = num_hidden + 1 #passes between layers\n",
    "    \n",
    "    # Initialise network: weighted outputs of nodes Z\n",
    "    Z_list = [X]\n",
    "    for i in range(num_hidden):\n",
    "        Z_list.append(np.zeros((hidden[i], num_obs)))\n",
    "    Z_list.append(np.zeros((num_output, num_obs)))\n",
    "    \n",
    "    # Initialise network: weights W:\n",
    "    hidden_init, output_init = get_init(hidden_activation), get_init(output_activation)\n",
    "    W_list =  [hidden_init((hidden[0], num_features))]\n",
    "    for i in range(1, num_hidden):\n",
    "        W_list.append(hidden_init((hidden[i], hidden[i-1])))\n",
    "    W_list.append(output_init((num_output, hidden[num_hidden-1])))\n",
    "    \n",
    "    # Initialise network: biases B\n",
    "    B_list = []\n",
    "    for i in range(num_hidden):\n",
    "        B_list.append(np.zeros((hidden[i],1)))\n",
    "    B_list.append(np.zeros((num_output, 1)))\n",
    "    \n",
    "    # Initalise network: activation + cost funcs and their derivatives\n",
    "    activation_hidden, activation_hidden_deriv = get_activation(hidden_activation)\n",
    "    activation_output, activation_output_deriv = get_activation(output_activation)\n",
    "    cost_func, cost_deriv = get_cost(loss)\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    iter = 0\n",
    "    diff = np.inf\n",
    "    while (iter <= max_iter and diff > tol):\n",
    "        # Compute forward pass\n",
    "        Z_list, A_list = forward_pass(X=X,\n",
    "                                      W_list=copy.deepcopy(W_list),\n",
    "                                      B_list=copy.deepcopy(B_list),\n",
    "                                      activation_hidden=activation_hidden,\n",
    "                                      activation_output=activation_output,\n",
    "                                      num_pass=num_pass)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = cost_func(A_list[num_pass], Y) #individual costs\n",
    "        \n",
    "        # Backpropagate to find gradients\n",
    "        batch_indices = list(np.random.choice(num_obs, batch_size, replace=False))\n",
    "        dW_list, dB_list = back_pass(cost_deriv=cost_deriv,\n",
    "                                     Y=copy.deepcopy(Y[:,batch_indices]),\n",
    "                                     activation_output_deriv=activation_output_deriv,\n",
    "                                     activation_hidden_deriv=activation_hidden_deriv,\n",
    "                                     W_list=copy.deepcopy(W_list),\n",
    "                                     A_list=copy.deepcopy([A[:,batch_indices] for A in A_list]),\n",
    "                                     Z_list=copy.deepcopy([Z[:,batch_indices] for Z in Z_list]),\n",
    "                                     num_pass=num_pass,\n",
    "                                     num_obs=batch_size)\n",
    "        \n",
    "        # Update weights (w_new = w_old - learn_rate * gradient(w_old))\n",
    "        W_old = copy.deepcopy(W_list)\n",
    "        B_old = copy.deepcopy(B_list)\n",
    "        new_diff = 0\n",
    "        for i in range(num_pass):\n",
    "            W_list[i] = W_list[i] - (learn_rate * dW_list[i]) #(learn_rate * dW_list[i])\n",
    "            B_list[i] = B_list[i] - (learn_rate * dB_list[i])\n",
    "            new_diff += np.sum(np.abs(W_list[i] - W_old[i])) + np.sum(np.abs(B_list[i] - B_old[i]))\n",
    "        diff = new_diff\n",
    "\n",
    "        # Increment iterations\n",
    "        iter += 1\n",
    "    \n",
    "    # Predicted values of the network\n",
    "    prediction = A_list[num_pass].T\n",
    "    \n",
    "    # Prediction function\n",
    "    def fnn_pred_func(X):\n",
    "        X = X.T,\n",
    "        _, A_list = forward_pass(X=X,\n",
    "                                      W_list=copy.deepcopy(W_list),\n",
    "                                      B_list=copy.deepcopy(B_list),\n",
    "                                      activation_hidden=activation_hidden,\n",
    "                                      activation_output=activation_output,\n",
    "                                      num_pass=num_pass)\n",
    "        return A_list[num_pass]\n",
    "    \n",
    "    # Return Yhat\n",
    "    return prediction, iter, diff, np.sum(cost), fnn_pred_func\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02]\n",
      " [0.98]\n",
      " [0.98]\n",
      " [0.02]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "pred, iter, diff, cost, func = fnn(X=X,\n",
    "                                   Y=Y,\n",
    "                                   hidden=[2],\n",
    "                                   learn_rate=0.9,\n",
    "                                   batch_size=4,\n",
    "                                   hidden_activation=\"sigmoid\",\n",
    "                                   output_activation=\"sigmoid\",\n",
    "                                   loss=\"mse\",\n",
    "                                   max_iter=10000)\n",
    "print(np.round(pred,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.94134682]\n",
      "  [0.74656039]]]\n"
     ]
    }
   ],
   "source": [
    "print(func(np.array([[0,1]])))\n",
    "# print(func(np.array([[1,1]])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b3010db54221e39668d7fe34b06214f54669d432870c9edb2f6a865eeb971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
