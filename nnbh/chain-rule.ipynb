{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain Rule\n",
    "\n",
    "The chain rule is used in calculus to compute the derivatives of composite functions.\n",
    "\n",
    "This section is heavily derived from [AI explained](https://explained.ai/matrix-calculus/#sec:1.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Calculus\n",
    "\n",
    "### Function Composition\n",
    "\n",
    "We start with two terms expressing two functions:\n",
    "\n",
    "$$ z = g(x) $$\n",
    "\n",
    "and:\n",
    "\n",
    "$$ y = f(g(x)) $$\n",
    "\n",
    "The second term represents a *composite* function. In other words, it's a combination of two functions.\n",
    "\n",
    "A composite function is sometimes written as:\n",
    "\n",
    "$$ f(g(x)) = (f \\circ g)(x) $$\n",
    "\n",
    "### Scalar Chain Rule\n",
    "\n",
    "For scalar values and functions, the chain can be given as:\n",
    "\n",
    "**Leibniz Notation**\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{dy}{dz} \\frac{dz}{dx} $$\n",
    "\n",
    "**Lagrange Notation**\n",
    "\n",
    "$$ (f \\circ g)'(x) = f'(g(x))g'(x) $$\n",
    "\n",
    "What these formulae state is that the derivative of the composite function as a whole is given by the product of the dirivative of the two functions of which it is comprised."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Calculus\n",
    "\n",
    "In the worlds of statistics and machine learning, we are not however typically dealing with scalar values. We need functions that can pack more punch.\n",
    "\n",
    "### Partial Derivatives\n",
    "\n",
    "- dervatives of functions with multiple variables\n",
    "\n",
    "consider the function\n",
    "\n",
    "$$ f(x,z) = 3x^2z $$\n",
    "\n",
    "we can take the dervative with respect to x:\n",
    "\n",
    "$$ \\frac{\\delta f(x,z)}{\\delta x} = 6xz $$\n",
    "\n",
    "and with respect to z:\n",
    "\n",
    "$$ \\frac{\\delta f(x,z)}{\\delta z} = 3x^2 $$\n",
    "\n",
    "### Motivating Vectors\n",
    "\n",
    "We move to the use of vectors one we want to begin organising our derviatives. In the case above, let's say we want to organise the derivatives with respect to $x$ abd $z$ into a vector. We call this vector the *gradient* of $f(x,z)$ and denote it as:\n",
    "\n",
    "$$\n",
    "\\Delta f(x,z) = \\begin{bmatrix} \\frac{\\delta f(x,z)}{\\delta x} & \\frac{\\delta f(x,z)}{\\delta z} \\end{bmatrix} = \\begin{bmatrix} 6xy & 3x^2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Calculus\n",
    "\n",
    "With vector calculus, we moved from one variable to many variables. With matrix calculus, we are now introducing the additional complication of more than function.\n",
    "\n",
    "In addition to the $f(x,z)$ above, we'll now also introduce:\n",
    "\n",
    "$$ g(x,z) = 2x + y^8 $$\n",
    "\n",
    "### Jacobian Matrices\n",
    "\n",
    "As before, we'll store the partial derivatives of each function in a vector. The difference this time is that these gradient vectors will form the rows of a matrix. This matrix is called a *Jacobian Matrix* (if you've heard of a *Jacobian* before, that's typically the determinant of this matrix). We can represent it as below:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{J} = \\begin{bmatrix} \\Delta f(x,z) \\\\ \\Delta g(x,z) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\delta f(x,z)}{\\delta x} & \\frac{\\delta f(x,z)}{\\delta z} \\\\ \\frac{\\delta g(x,z)}{\\delta x} & \\frac{\\delta g(x,z)}{\\delta z} \\end{bmatrix} = \\begin{bmatrix} 6xz & 3x^2 \\\\ 2 & 8z^7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that this matrix is represented in the *numerator* layout (i.e. the numerator of the derivative is on the rows). However, Jacobian matrices are also often organised in the *denominator* layout, which is the transpose of the numerator layout.\n",
    "\n",
    "### Generalised Jacobian Matrices\n",
    "\n",
    "Let's say we have the following $n$ variables:\n",
    "\n",
    "$$ \\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$\n",
    "\n",
    "And let's also say we have $m$ functions through which each feature is passed:\n",
    "\n",
    "$$ \\begin{matrix} y_1 = f_1(\\boldsymbol{x}) \\\\ y_2 = f_2(\\boldsymbol{x}) \\\\ \\vdots \\\\ y_m = f_m(\\boldsymbol{x}) \\end{matrix} $$\n",
    "\n",
    "The Jacboian matrix is then the collection of all $m \\times n$ partial derviatives, or the stack of all $m$ gradient vectors with respect to $\\boldsymbol{x}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{J} = \\frac{\\delta \\boldsymbol{y}}{\\delta \\boldsymbol{x}} = \\begin{bmatrix} \\Delta f_1(\\boldsymbol{x}) \\\\ \\Delta f_2(\\boldsymbol{x}) \\\\ \\vdots \\\\ \\Delta f_m(\\boldsymbol{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\delta}{\\delta x_1} f_1(\\boldsymbol{x}) & \\frac{\\delta}{\\delta x_2} f_1(\\boldsymbol{x}) & \\ldots & \\frac{\\delta}{\\delta x_n} f_1(\\boldsymbol{x}) \\\\ \\frac{\\delta}{\\delta x_1} f_2(\\boldsymbol{x}) & \\frac{\\delta}{\\delta x_2} f_2(\\boldsymbol{x}) & \\ldots & \\frac{\\delta}{\\delta x_n} f_2(\\boldsymbol{x}) \\\\ & \\vdots & & \\\\ \\frac{\\delta}{\\delta x_1} f_m(\\boldsymbol{x}) & \\frac{\\delta}{\\delta x_2} f_m(\\boldsymbol{x}) & \\ldots & \\frac{\\delta}{\\delta x_n} f_m(\\boldsymbol{x}) \\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b3010db54221e39668d7fe34b06214f54669d432870c9edb2f6a865eeb971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
